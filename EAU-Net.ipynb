{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9941027,"sourceType":"datasetVersion","datasetId":5174487},{"sourceId":9947545,"sourceType":"datasetVersion","datasetId":4054272}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport cv2\nfrom glob import glob\nimport tensorflow as tf\nimport tf_keras as keras\nimport keras.backend as K\nimport tf_keras.layers as L\nfrom tf_keras import backend as K\nfrom tf_keras.models import Model\nfrom tf_keras.layers import Input, Conv2D, MaxPooling2D, MaxPool2D, Add, Dropout, Concatenate,concatenate, Conv2DTranspose, Dense, Reshape, Flatten, Softmax, Lambda, UpSampling2D, AveragePooling2D, Activation, BatchNormalization, GlobalAveragePooling2D, SeparableConv2D,Multiply\nfrom tf_keras.optimizers import Adam\nfrom tf_keras.metrics import MeanIoU\nfrom tf_keras.utils import to_categorical\nfrom tf_keras.metrics import BinaryAccuracy, Precision, Recall\nfrom tf_keras.applications import ResNet50\nfrom sklearn.model_selection import train_test_split\nfrom tf_keras.preprocessing.image import load_img, img_to_array\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Building the proposed model ( U-Net (MobileNetV2 as encoder) + Effective Edge Detection (EED) technique + Bit-Plane Attention (BPA) technique + Combined loss of dice loss and BCE loss )","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import Conv2D, Input, UpSampling2D, concatenate, GlobalAveragePooling2D, Reshape, Dense, Multiply, Activation, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import ReLU, Add\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# BitPlaneAttention Block\ndef BitPlaneAttention(input_tensor):\n    channels = input_tensor.shape[-1]\n    x = GlobalAveragePooling2D()(input_tensor)\n    x = Dense(channels // 4, activation='relu')(x)\n    x = Dense(channels, activation='sigmoid')(x)\n    x = Reshape((1, 1, channels))(x)\n    return Multiply()([input_tensor, x])\n\n# Edge Prediction Network\nclass EdgePredictionNet(tf.keras.Model):\n    def __init__(self, in_channels):\n        super(EdgePredictionNet, self).__init__()\n        self.conv1 = layers.Conv2D(64, kernel_size=3, padding='same', input_shape=(None, None, in_channels))\n        self.conv2 = layers.Conv2D(32, kernel_size=3, padding='same')\n        self.conv3 = layers.Conv2D(1, kernel_size=1, padding='same')  # Single channel for edge map\n        self.relu = layers.ReLU()\n\n    def call(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        edge_map = tf.nn.sigmoid(self.conv3(x))  # Normalize edge map to [0, 1]\n        return edge_map\n\nclass EdgeRefinementModule(tf.keras.Model):\n    def __init__(self, in_channels):\n        super(EdgeRefinementModule, self).__init__()\n        self.conv1 = layers.Conv2D(64, kernel_size=3, padding='same')\n        self.conv2 = layers.Conv2D(32, kernel_size=3, padding='same')\n        self.conv3 = layers.Conv2D(1, kernel_size=1, padding='same')  # Refined edge map\n        self.relu = layers.ReLU()\n\n    def call(self, x, edge_map):\n        # Concatenate feature map and edge map\n        refined_input = tf.concat([x, edge_map], axis=-1)\n        x = self.relu(self.conv1(refined_input))\n        x = self.relu(self.conv2(x))\n        refined_edge_map = tf.nn.sigmoid(self.conv3(x))\n        return refined_edge_map\n\nclass NeuralEdgeRefinement(tf.keras.Model):\n    def __init__(self, in_channels):\n        super(NeuralEdgeRefinement, self).__init__()\n        self.epn = EdgePredictionNet(in_channels)\n        self.refinement = EdgeRefinementModule(in_channels)\n\n    def call(self, x):\n        # Step 1: Predict initial edge map\n        coarse_edge_map = self.epn(x)\n        # Step 2: Refine the edge map\n        refined_edge_map = self.refinement(x, coarse_edge_map)\n        return refined_edge_map\n\nclass EdgeGuidedAttention(tf.keras.Model):\n    def __init__(self, in_channels, out_channels):\n        super(EdgeGuidedAttention, self).__init__()\n        self.refinement = NeuralEdgeRefinement(in_channels)\n        self.spatial_attention = layers.Conv2D(1, kernel_size=3, padding='same')\n\n    def call(self, x):\n        # Get refined edge map\n        refined_edge_map = self.refinement(x)\n        # Compute spatial attention weights\n        attention_weights = tf.nn.sigmoid(self.spatial_attention(refined_edge_map))\n        # Apply attention\n        output = x * attention_weights\n        return output, refined_edge_map\n\n\n# U-Net with MobileNetV2 and BitPlaneAttention\ndef unet_with_mobilenet_and_bitplane(input_shape=(512, 512, 3), num_classes=1, activation='relu', output_activation='sigmoid', batch_norm=True):\n    inputs = Input(shape=input_shape)\n\n    # Use MobileNetV2 as the encoder (backbone)\n    mobilenet = MobileNetV2(input_tensor=inputs, include_top=False, weights='imagenet')\n\n    # Extract feature maps at different stages\n    conv1 = mobilenet.get_layer('block_1_expand_relu').output   # Size: 256x256\n    conv2 = mobilenet.get_layer('block_3_expand_relu').output   # Size: 128x128\n    conv3 = mobilenet.get_layer('block_6_expand_relu').output   # Size: 64x64\n    conv4 = mobilenet.get_layer('block_13_expand_relu').output  # Size: 32x32\n    center = mobilenet.get_layer('block_16_project').output     # Size: 16x16\n\n    # Apply BitPlaneAttention to the bottleneck\n    center = BitPlaneAttention(center)\n    center, _ = EdgeGuidedAttention(in_channels=center.shape[-1], out_channels=center.shape[-1])(center)\n\n    # Decoder part (upsampling) with BitPlaneAttention and Edge Refinement in skip connections\n    up4 = UpSampling2D(size=(2, 2))(center)\n    up4 = concatenate([up4, conv4], axis=-1)\n    up4 = BitPlaneAttention(up4)\n    up4, _ = EdgeGuidedAttention(in_channels=up4.shape[-1], out_channels=up4.shape[-1])(up4)\n    up4 = Conv2D(512, (3, 3), padding='same', activation=activation)(up4)\n    if batch_norm:\n        up4 = BatchNormalization()(up4)\n\n    up3 = UpSampling2D(size=(2, 2))(up4)\n    up3 = concatenate([up3, conv3], axis=-1)\n    up3 = BitPlaneAttention(up3)\n    up3, _ = EdgeGuidedAttention(in_channels=up3.shape[-1], out_channels=up3.shape[-1])(up3)\n    up3 = Conv2D(256, (3, 3), padding='same', activation=activation)(up3)\n    if batch_norm:\n        up3 = BatchNormalization()(up3)\n\n    up2 = UpSampling2D(size=(2, 2))(up3)\n    up2 = concatenate([up2, conv2], axis=-1)\n    up2 = BitPlaneAttention(up2)\n    up2, _ = EdgeGuidedAttention(in_channels=up2.shape[-1], out_channels=up2.shape[-1])(up2)\n    up2 = Conv2D(128, (3, 3), padding='same', activation=activation)(up2)\n    if batch_norm:\n        up2 = BatchNormalization()(up2)\n\n    up1 = UpSampling2D(size=(2, 2))(up2)\n    up1 = concatenate([up1, conv1], axis=-1)\n    up1 = BitPlaneAttention(up1)\n    up1, _ = EdgeGuidedAttention(in_channels=up1.shape[-1], out_channels=up1.shape[-1])(up1)\n    up1 = Conv2D(64, (3, 3), padding='same', activation=activation)(up1)\n    if batch_norm:\n        up1 = BatchNormalization()(up1)\n\n    # Final Upsampling to match the input size\n    final_upsample = UpSampling2D(size=(2, 2))(up1)\n    outputs = Conv2D(num_classes, (1, 1), activation=output_activation)(final_upsample)\n\n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\n# Example usage\n#model = unet_with_mobilenet_and_bitplane()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Definition of Evaluation metrics","metadata":{}},{"cell_type":"code","source":"def dice_score(y_true, y_pred):\n    smooth = K.epsilon()\n    y_true_flat = K.flatten(K.cast(y_true, 'float32'))\n    y_pred_flat = K.flatten(y_pred)\n    intersection = K.sum(y_true_flat * y_pred_flat)\n    score = (2. * intersection + smooth) / (K.sum(y_true_flat) + K.sum(y_pred_flat) + smooth)\n    return score\n\ndef iou(y_true, y_pred):\n    smooth = K.epsilon()\n    y_true_flat = K.flatten(K.cast(y_true, 'float32'))\n    y_pred_flat = K.flatten(y_pred)\n    intersection = K.sum(y_true_flat * y_pred_flat)\n    union = K.sum(y_true_flat) + K.sum(y_pred_flat) - intersection + smooth\n    iou = (intersection + smooth) / union\n    return iou\n\ndef recall(y_true, y_pred):\n    smooth = K.epsilon()\n    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n    y_true_flat = K.flatten(K.cast(y_true, 'float32'))\n    y_pred_flat = K.flatten(y_pred_pos)\n    tp = K.sum(y_true_flat * y_pred_flat)\n    fn = K.sum(y_true_flat * (1 - y_pred_flat))\n    recall = (tp + smooth) / (tp + fn + smooth)\n    return recall\n\ndef precision(y_true, y_pred):\n    smooth = K.epsilon()\n    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n    y_true_flat = K.flatten(K.cast(y_true, 'float32'))\n    y_pred_flat = K.flatten(y_pred_pos)\n    tp = K.sum(y_true_flat * y_pred_flat)\n    fp = K.sum((1 - y_true_flat) * y_pred_flat)\n    precision = (tp + smooth) / (tp + fp + smooth)\n    return precision","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Definition of loss function","metadata":{}},{"cell_type":"code","source":"def dice_loss(y_true, y_pred):\n    loss = 1 - dice_score(y_true, y_pred)\n    return loss\n\ndef bce_loss(y_true, y_pred):\n    y_true = tf.cast(y_true, tf.float32)\n    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred))\n    return loss\n\n# Hybrid Loss\ndef combined_loss(y_true, y_pred):\n    dice = dice_loss(y_true, y_pred)\n    bce = bce_loss(y_true, y_pred)\n    total_loss = dice+bce\n    return total_loss\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Define the input shape and number of classes for the model\ninput_shape = (512, 512, 3)  # Example input shape for grayscale images\nnum_classes = 1  # Binary segmentation","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Building the model and compiling it","metadata":{}},{"cell_type":"code","source":"#model = unet_model()\nfrom tensorflow.keras.optimizers import Adam\n\nmodel = unet_with_mobilenet_and_bitplane(input_shape, num_classes)\noptimizer =tf.keras.optimizers.Nadam(learning_rate=0.00001)\nmodel.compile(loss=combined_loss, metrics=[\"accuracy\", dice_score, recall, precision, iou], optimizer=optimizer)\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Dataloader for ISIC 2016","metadata":{}},{"cell_type":"code","source":"\nclass ISIC_2016:\n    def __init__(self, data_dir, image_size=(256, 256), batch_size=8, mode='train'):\n        self.data_dir = data_dir\n        self.image_size = image_size\n        self.batch_size = batch_size\n        self.mode = mode\n\n        # Load CSV files based on the mode\n        if self.mode == 'train':\n            csv_filename = \"/kaggle/input/isic-2016-dataset/train_ISIC_2016.csv\"\n        elif self.mode == 'val':\n            csv_filename = \"/kaggle/input/isic-2016-dataset/val_ISIC_2016.csv\"\n        elif self.mode == 'test':\n            csv_filename = \"/kaggle/input/isic-2016-dataset/test_ISIC_2016.csv\"\n\n        self.csv_path = os.path.join(self.data_dir, csv_filename)\n        self.df = pd.read_csv(self.csv_path)\n\n\n        if self.mode == 'test':\n        # Define image and mask paths\n            self.image_path = os.path.join(self.data_dir, \"ISBI2016_ISIC_Part1_Test_Data\")\n            self.mask_path = os.path.join(self.data_dir, \"ISBI2016_ISIC_Part1_Test_GroundTruth\")\n        else:\n            self.image_path = os.path.join(self.data_dir, \"ISBI2016_ISIC_Part1_Training_Data\")\n            self.mask_path = os.path.join(self.data_dir, \"ISBI2016_ISIC_Part1_Training_GroundTruth\")\n\n        # Initialize the current batch index to 0\n        self.current_batch_index = 0\n\n    def __len__(self):\n        return int(np.ceil(len(self.df) / float(self.batch_size)))\n\n    def __iter__(self):\n        while self.current_batch_index < len(self.df):\n            batch_images = []\n            batch_masks = []\n            \n            for i in range(self.current_batch_index, min(self.current_batch_index + self.batch_size, len(self.df))):\n                image_name = self.df.iloc[i]['Image_Id']\n                mask_name = self.df.iloc[i]['Image_Id'][:-4]+'_Segmentation'+'.png'\n\n                image = load_img(os.path.join(self.image_path, image_name), target_size=self.image_size)\n                mask = load_img(os.path.join(self.mask_path, mask_name), target_size=self.image_size, color_mode='grayscale')\n\n                image_arr = img_to_array(image) / 255.0\n                mask_arr = img_to_array(mask) / 255.0\n\n                batch_images.append(image_arr)\n                batch_masks.append(mask_arr)\n\n            batch_images = np.array(batch_images)\n            batch_masks = np.array(batch_masks)\n\n            # Update the current batch index for the next iteration\n            self.current_batch_index += self.batch_size\n\n            yield batch_images, batch_masks\n\n        # Reset the current batch index at the end of one epoch\n        self.current_batch_index = 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Dataloader for cross dataset ( ISIC 2016 train set for training, ISIC 2016 validation set for validation and PH2 test set for testing","metadata":{}},{"cell_type":"code","source":"\nclass ISIC_2016_PH2:\n    def __init__(self, data_dir, image_size=(256, 256), batch_size=8, mode='train'):\n        self.data_dir = data_dir\n        self.image_size = image_size\n        self.batch_size = batch_size\n        self.mode = mode\n\n        # Load CSV files based on the mode\n        if self.mode == 'train':\n            csv_filename = \"/kaggle/input/isic-2016-dataset/train_ISIC_2016.csv\"\n        elif self.mode == 'val':\n            csv_filename = \"/kaggle/input/isic-2016-dataset/val_ISIC_2016.csv\"\n        elif self.mode == 'test':\n            csv_filename = \"/kaggle/input/isic-2017-segmentation/test_ph2.csv\"\n\n        self.csv_path = os.path.join(self.data_dir, csv_filename)\n        self.df = pd.read_csv(self.csv_path)\n\n\n        if self.mode == 'test':\n        # Define image and mask paths\n            # Define image and mask paths\n            self.image_path = os.path.join(self.data_dir, \"Images\")\n            self.mask_path = os.path.join(self.data_dir, \"Masks\")\n        else:\n            self.image_path = os.path.join(self.data_dir, \"ISBI2016_ISIC_Part1_Training_Data\")\n            self.mask_path = os.path.join(self.data_dir, \"ISBI2016_ISIC_Part1_Training_GroundTruth\")\n\n        # Initialize the current batch index to 0\n        self.current_batch_index = 0\n\n    def __len__(self):\n        return int(np.ceil(len(self.df) / float(self.batch_size)))\n\n    def __iter__(self):\n        while self.current_batch_index < len(self.df):\n            batch_images = []\n            batch_masks = []\n            if self.mode =='test':\n                \n                for i in range(self.current_batch_index, min(self.current_batch_index + self.batch_size, len(self.df))):\n                    image_name = self.df.iloc[i]['Image_Name']\n                    mask_name = self.df.iloc[i]['Image_Name'][:-4]+'_lesion'+'.bmp'\n    \n                    image = load_img(os.path.join(self.image_path, image_name), target_size=self.image_size)\n                    mask = load_img(os.path.join(self.mask_path, mask_name), target_size=self.image_size, color_mode='grayscale')\n    \n                    image_arr = img_to_array(image) / 255.0\n                    mask_arr = img_to_array(mask) / 255.0\n    \n                    batch_images.append(image_arr)\n                    batch_masks.append(mask_arr)\n            else:\n                for i in range(self.current_batch_index, min(self.current_batch_index + self.batch_size, len(self.df))):\n                    image_name = self.df.iloc[i]['Image_Id']\n                    mask_name = self.df.iloc[i]['Image_Id'][:-4]+'_Segmentation'+'.png'\n    \n                    image = load_img(os.path.join(self.image_path, image_name), target_size=self.image_size)\n                    mask = load_img(os.path.join(self.mask_path, mask_name), target_size=self.image_size, color_mode='grayscale')\n    \n                    image_arr = img_to_array(image) / 255.0\n                    mask_arr = img_to_array(mask) / 255.0\n    \n                    batch_images.append(image_arr)\n                    batch_masks.append(mask_arr)\n\n\n            batch_images = np.array(batch_images)\n            batch_masks = np.array(batch_masks)\n\n            # Update the current batch index for the next iteration\n            self.current_batch_index += self.batch_size\n\n            yield batch_images, batch_masks\n\n        # Reset the current batch index at the end of one epoch\n        self.current_batch_index = 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Calling the dataset loader function","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport tensorflow as tf\n#from some_module import ISIC_2016  # Replace with the actual module if different\n\n# Initialize the data loaders\ntrain_data_loader = ISIC_2016_PH2(data_dir='/kaggle/input/isic-2016-dataset/ISIC_2016_dataset/ISIC_2016_dataset', image_size=(512, 512), batch_size=4, mode='train')\nval_data_loader = ISIC_2016_PH2(data_dir='/kaggle/input/isic-2016-dataset/ISIC_2016_dataset/ISIC_2016_dataset', image_size=(512, 512), batch_size=4, mode='val')\ntest_data_loader = ISIC_2016_PH2(data_dir='/kaggle/input/isic-2017-segmentation/Arranged_PH2_dataset/Arranged_PH2_dataset', image_size=(512, 512), batch_size=4, mode='test')\n\n# Create the datasets\ntrain_dataset = tf.data.Dataset.from_generator(\n    lambda: train_data_loader,\n    output_signature=(\n        tf.TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(None, 512, 512, 1), dtype=tf.float32)\n    )\n).repeat()  # Add repeat() to the dataset\n\nval_dataset = tf.data.Dataset.from_generator(\n    lambda: val_data_loader,\n    output_signature=(\n        tf.TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(None, 512, 512, 1), dtype=tf.float32)\n    )\n).repeat()  # Add repeat() to the dataset\n\ntest_dataset = tf.data.Dataset.from_generator(\n    lambda: test_data_loader,\n    output_signature=(\n        tf.TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(None, 512, 512, 1), dtype=tf.float32)\n    )\n).repeat()  # Add repeat() to the dataset\n\n# Calculate steps per epoch\ntrain_steps_per_epoch = len(train_data_loader) // train_data_loader.batch_size\nval_steps_per_epoch = len(val_data_loader) // val_data_loader.batch_size\n\n# Define the model (example)\n#model = tf.keras.models.Sequential([\n#    tf.keras.layers.InputLayer(input_shape=(512, 512, 3)),\n#    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n#    tf.keras.layers.MaxPooling2D((2, 2)),\n#    tf.keras.layers.Flatten(),\n#    tf.keras.layers.Dense(10, activation='softmax')3])\n\n# Compile the model\n#model.compile(optimizer='Nadam', loss=combined_loss, metrics=[\"accuracy\", dice_score, recall, precision, iou])\n\n#model.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Model Training","metadata":{}},{"cell_type":"code","source":"# Create a callback that saves the model's weights\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath='/kaggle/working/model_weight.weights.h5',  # Where to save the model file\n    save_weights_only=True,  # Only save the model's weights\n    monitor='val_dice_score',  # Metric to monitor\n    mode='max',  # Mode can be 'min', 'max', or 'auto'\n    save_best_only=True  # Save only the best model\n)\n\n# Train the model with the checkpoint callback\nhistory=model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    steps_per_epoch=train_steps_per_epoch,\n    validation_steps=val_steps_per_epoch,\n    epochs=500,\n    callbacks=[checkpoint_callback]  # Include the checkpoint callback\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Loading the best weights of parameters of the proposed model after training","metadata":{}},{"cell_type":"code","source":"# Load the best weights later\nmodel.load_weights('/kaggle/working/model_weight.weights.h5')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_steps_per_epoch = len(test_data_loader) // test_data_loader.batch_size","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Evaluation on the test set","metadata":{}},{"cell_type":"code","source":"# Evaluate the model on test data\nmodel.evaluate(test_dataset, steps=test_steps_per_epoch)\n\n# Make predictions on new data\npredictions = model.predict(test_dataset, steps=test_steps_per_epoch)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Displaying the accuracy curve","metadata":{}},{"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.figure(figsize=(12, 6))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Displaying the loss curve","metadata":{}},{"cell_type":"code","source":"# Plot training & validation loss values\nplt.figure(figsize=(12, 6))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}